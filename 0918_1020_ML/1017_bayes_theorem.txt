Bayes Theorem 베이즈 정리
넷플리스에서 사용하는 컨텐츠 추천 알고리즘

class : B, ㄱB(not B)
prob : T, ㄱT(not T)

prior prob : 사전 확률
P(B)

normalization constant : 정규화 상수
P(T)


사건==정보
Conditional probability 조건부 확률 : P(A|B)
1 ~ 100 -> P(A) = 1/100
정보B) 1의 자리가 5 : P(A|B) = 1/10
정보B) 짝수 : P(A|B) = 1/2


Likelihood : 가능성, 우도
클래스를 보고 현상을 예측 (미래의 현상 예측)
예) 어떤 고객이 매장에 들어왔을 때 구매o/구매x
P(T|B) : B클래스일때 T할 확률
P(ㄱT|B) : B클래스일때 not T할 확률
P(T|ㄱB) : not B클래스일때 T할 확률
P(ㄱT|ㄱB) : not B클래스일때 not T할 확률


posterior probability : 사후 확률. (Likelihood 와 방향이 반대)
현상을 보고 클래스를 예측 (과거?의 클래스 예측)
어떤 현상 -> 특정 class
P(B|T) : T를 보고 B의 확률

# Bayes Theorem
P(B|T) = (P(T|B) * P(B))/P(T)
(posterior probability) = ((Likelihood)*(prior prob))/(normalization constant)



ChatGPT
어떤 사건 B가 주어졌을 때 사건 T가 발생할 확률 P(B|T)를 계산하는데 사용됩니다. 베이즈 정리는 다음과 같이 표현됩니다:

P(B|T) = (P(T|B) * P(B)) / P(T)

여기에서 각 항목의 의미는 다음과 같습니다:

P(B|T): 사건 T가 주어졌을 때 사건 B가 발생할 조건부 확률.
P(T|B): 사건 B가 주어졌을 때 사건 T가 발생할 조건부 확률.
P(B): 사건 B의 사전 확률 (사건 B가 일어날 확률).
P(T): 사건 T의 사전 확률 (사건 T가 일어날 확률).
이 공식은 사건 T가 관측되었을 때, 사건 B의 확률을 업데이트하는 데 사용됩니다. 
베이즈 정리는 주로 베이지안 통계, 기계 학습, 데이터 분석 등 다양한 분야에서 확률적 추론에 활용되며, 
예를 들어 확률적 분류, 추정 및 예측 문제를 다룰 때 중요한 도구 중 하나로 활용됩니다.







그림을 그리며 이해하는게 빠르다.
구매 의지 o/x -> B/ㄱB (hypothesis = 가설)
말을 거는지 o/x -> T/ㄱT (evidence = 정보)

P(T|B) 를 읽을 때 '피티빠비'

prior prob : 사전 확률
구매할 확률 20% 
B : ㄱB= 2 : 8
P(B) = 0.2
P(ㄱB) = 0.8


Likelihood : 가능성, 우도
P(T|B) = 0.9 # 구매한 사람이 말을 건 확률
P(ㄱT|B) = 0.1 # 구매한 사람이 말을 안 건 확률
P(T|ㄱB) = 0.3 # 구매안한 사람이 말을 건 확률
P(ㄱT|ㄱB) = 0.7 # 구매안한 사람이 말을 안 건 확률


joint probability (n을 교집합 기호를 임의로 사용)
P(B n T) = P(B) * P(T|B) = 0.2*0.9    # 
P(B n ㄱT) = P(B) * P(ㄱT|B) = 0.2*0.1
P(ㄱB n T) = P(ㄱB) * P(T|ㄱB) =
P(ㄱB n ㄱT) = P(ㄱB) * P(ㄱT|ㄱB) = 


관측 정보 추가 : 말을 걸었다
그럼 아래 두개의 교집합의 확률을 전체 확률로 만든다. 
두개를 합한게 분모로 보내고 class를 분자로 두면 된다.
P(B n T) = 0.18
P(ㄱB n T) = 0.24

예시1 : 아무런 정보를 관측하지 못했을 때,
구매의사기 있을 확률.
P(B) = 0.2

예시2 : 말을 걸었다는 정보를 관측 되었다고 했을 때,
구매의사가 있을 확률이 갱신 된다.
P(B|T) = 0.18/(0.18 + 0.24) ~= 0.43


다시 공식을 보자.
P(B|T) = (P(T|B) * P(B)) / P(T)

P(T|B) * P(B) : P(B n T) # 분자 B의 확률 중 T가 발생할 확률
P(T) : P(B n T) + P(ㄱB n T) # T가 관측 되었으므로 T의 확률를 분모로 둔다.


말을 걸지 않았다는 정보를 관측 되었다고 한다면
구매의사 확률
P(B|ㄱT) = B의 확률중 ㄱT의 영역 / P(ㄱT) 
P(B) * P(ㄱT|B)/(P(BnㄱT)+P(BnㄱT))
0.2 * 0.1 / (P(B)*P(ㄱT|B) + P(ㄱB)*P(ㄱB n ㄱT))
0.02/(0.2*0.1 + 0.8*0.7)





prior probability 사전 확률
P(B) = 0.2

Conditional probability : 조건부 확률
P(T|B) = 0.9 # B의 조건을 만족하는 T의 확률


likelihood
P(T|B) = 0.9 # B class의 T 관측 확률
P(ㄱT|B) = 0.1 # B class의 not T 관측 확률
P(T|ㄱB) = 0.3 # not B class의 T 관측 확률
P(ㄱT|ㄱB) = 0.7 # not B class의 not T 관측 확률

posterior probability
P(B|T) = P(B n T) / P(T) 
B의 확률(영역)에서 T의 발생 확률(교집합) / (T의 확률)

joint probability
P(B n T) = P(B) * P(T|B) # P(B) 확률(영역)에서 T의 발생 비율.


P(T) = P(T|B) + P(T|ㄱB)

bayes Theorem
P(B|T) = (P(T|B) * P(B))/P(T)




### 연습문제1
상황 : 암 -> 양성
실제로 암에 걸린 사람이 양성이 나올 확률이 95%

검사했는데 양성이 나왔다면 암에 걸릴 확률이 95%인가?????????

암 T/F : C/~C
양/음 : P/~P

실제 암 -> 양성 : 95%
건강 -> 양성 : 2%
건강 -> 음성 : 98%
암 자연발생율 -> 0.1%

# prior probability : 암환자/정상인의 확률
P(C) = 0.001 # 암 자연 발생율
P(~C) = 0.999 # 암 자연 미발생율

# Likelihood : 암환자/정상인일 경우 양/음성이 검측될 확률
P(P|C) = 0.95 # 암환자의 양성 확률
P(~P|C) = 0.05 # 암환자의 음성 확률
P(P|~C) = 0.02 # 정상인의 양성 확률
P(~P|~C) = 0.98 # 정상인의 음성 확률

(joint probability 사용) 
# 사람(암환자+정상인)의 양/음성일 확률(두 영역의 교집합). 각각의 조건(C/~C)에서의 확률을 모든 조건(C + ~C)에서의 확률로 변환.
P(P n C) = P(C) * P(P|C) # 암환자(확률) * 암환자의 양성(확률) = 암환자의 양성 확률/(암환자+정상인) = 양성 암환자 확률
P(~P n C) = P(C) * P(~P|C) # 암환자 * 암환자의 음성 = 암환자의 음성 확률/(암환자+정상인) = 음성 암환자 확률
P(P n ~C) = P(~C) * P(P|~C) # 정상인 * 정상인의 양성 = 정상인의 양성 확률/(암환자+정상인) = 양성 정상인 확률
P(~P n ~C) = P(~C) * P(~P|~C) # 정상인 * 정상인의 음성 = 정상인의 음성 확률/(암환자+정상인) = 음성 정상인 확률

# P(P), P(~P) : 양/음성이 검측될 확률
P(P) : 양성 암환자 확률 + 양성 정상인 확률
P(~P) : 음성 암환자 확률 + 음성 정상인 확률
P(P) = P(P n C) + P(P n ~C) = (P(C) * P(P|C)) + (P(C) * P(P|~C)) = ((0.001 * 0.95) + (0.999 * 0.02)) = 0.02093
P(~P) = P(~P n C) + P(~P n ~C) = (P(C) * P(~P|C)) + (P(C) * P(~P|~C)) = ((0.001 * 0.05) + (0.999 * 0.98)) = 0.97907


# posterior probability : 양/음성이 검측되었을 때 암환자/정상인의 확률
P(C|P) : 양성이 검측되었을 경우 암환자일 확률 
    = 분모(양성일 확률), 분자(양성 암환자일 확률)
    = 양성일 확률 : P(P), 암환자일 확률 : P(P n C) -> P(C) * P(P|C)

P(C|P) = (P(P n C)/P(P)) = P(P|C) * P(C) / P(P) = (0.95 * 0.001)/0.02093
P(~C|P) = (0.02 * 0.999)/0.02093
P(C|~P) = (0.05 * 0.001)/0.97907
P(~C|~P) = (0.98 * 0.999)/0.97907

0.04538939321548017
0.9546106067845198
5.1068871480078036e-05
0.9999489311285199

목표 : 양성일 경우 실제 암일 확율
0.04538939321548017




### 연습문제2
스팸:정상 메일의 비율 = 4:6

스팸 : S/~S
A단어 포함 : A/~A
P(S) = 0.4
P(~S) = 0.6

P(A|S) = 0.25
P(~A|S) = 0.75
P(A|~S) = 0.02
P(~A|~S) = 0.98

P(S|A) = ?

P(A) = P(A n S) + P(A n ~S) = P(S)*P(A|S) + P(~S)*P(A|~S) = 0.6*0.25 + 0.6*0.02 = 0.162

P(S|A) = P(A|S)*P(S)/P(A) = 0.25*0.4/0.162 = 0.6172839506172839

### 연습문제3
P(A) = 0.1




----- 간단한 예시.
교실바닥에 긴머리카락 발견 -> 여학생의 것을 추측 : prior -> MLE

정보 추가) 교실 학생의 남학생의 절대 다수가 긴머리 -> 남학생의 것일 수도 있다. : bayes update(prior + likelihood) = posterior : MAP